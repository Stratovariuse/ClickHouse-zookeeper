https://clickhouse.com/docs/ru/engines/table-engines/mergetree-family/replication
https://clickhouse.com/docs/ru/development/architecture#replication

Установить зукипер (в примере он 1)

```
<zookeeper incl="zookeeper-servers" optional="true" />
<macros incl="macros" optional="true" />
```

```
<?xml version="1.0"?>
<clickhouse>
    <clickhouse_remote_servers>
        <mycluster> // Название кластера
            <shard>
                <internal_replication>true</internal_replication>
                <replica>
                    <host>167.99.142.32</host> // Первая нода одного шарда
                    <port>9000</port>
                </replica>
                <replica>
                    <host>159.65.123.161</host> // Вторая нода одного шарда
                    <port>9000</port>
                </replica>
            </shard>
        </mycluster>
    </clickhouse_remote_servers>

    <zookeeper-servers>
        <node index="1">
            <host>159.65.119.28</host> // сервер зукипер
            <port>2181</port> // порт для подключения
        </node>
    </zookeeper-servers>

    <macros>
		<cluster>mycluster</cluster> // название кластера
        <replica>167.99.142.32</replica> // Адрес данной машины
        <shard>01</shard> // Шард данной машины
    </macros>
</clickhouse>
```

Параметр internal_replication=false отвечает за то, будет ли дублировать ClickHouse запросы на запись или редактирование данных в distributed-таблице автоматически на все реплики. То есть когда вы вставляете данные в distributed таблицу, она сама может увидеть все реплики в шарде и продублировать запрос на вставку на все реплики этого шарда.

Второй же вариант - это настройка внутренних механизмов репликации internal_replication=true. В этом случае, данные при запросе вставляются в одну таблицу, а потом реплицируются внутренними механизмами*. В данном варианте требуется установка zookeeper - это распределенное key-value хранилище, которое clickhouse будет использовать для хранения состояния репликации и решения конфликтов.

Предпочтительным является второй вариант т.к. он гарантирует целостность. В первом случае запросы просто дублируются на каждую таблицу и целостность не гарантируется.

## Создание реплицированных таблиц

```
CREATE TABLE if not exists ch_replicated_local ON CLUSTER local
(
    id Int64,
    title String,
    description String,
    content String,
    date Date
)
ENGINE = ReplicatedMergeTree('/clickhouse/{cluster}/tables/ch_replicated_local', '{replica}')
PARTITION BY date
ORDER BY id;
```

В параметрах мы указываем путь к таблице в Zookeeper, а также название реплики. Реплицируемые таблицы должны иметь одинаковый путь в Zookeeper. То есть если вы хотите чтобы 5 нод реплицировали одну и ту же таблицу - то путь в зукипере у них будет один и тот же - /clickhouse/{cluster}/tables/posts_replicated, а вот индекс реплики будет отличаться.

Значения переменных {cluster} и {replica} автоматически подставятся для каждой ноды на кластере из макросов

## Создание Distributed таблиц

```
 CREATE TABLE if not exists ch_replicated_distributed ON CLUSTER local
(
    id Int64,
    title String,
    description String,
    content String,
    date Date
)
ENGINE = Distributed('{cluster}', 'default', 'ch_replicated_local', rand());
```

